# llama.cpp Frontend

A modern Next.js frontend for interacting with llama.cpp models.

## Features

- ðŸŽ¨ Dark theme UI matching llama.cpp style
- ðŸ’¬ Real-time streaming chat interface
- ðŸ”„ KV cache slot tracking
- ðŸ“± Responsive design
- âš¡ Built with Next.js 15, React 18, and Tailwind CSS

## Getting Started

### Prerequisites

- Node.js 18+ 
- npm or yarn
- Backend server running on `http://localhost:8080`

### Installation

```bash
npm install
```

### Development

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

### Configuration

The frontend connects to the backend API at `http://localhost:8080` by default. You can change this by creating a `.env.local` file:

```env
NEXT_PUBLIC_API_URL=http://localhost:8080
```

### Build

```bash
npm run build
npm start
```

## Project Structure

```
frontend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ChatInterface.tsx    # Main chat UI component
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ api.ts                # API utilities for backend communication
â”‚   â”œâ”€â”€ globals.css               # Global styles
â”‚   â”œâ”€â”€ layout.tsx                # Root layout
â”‚   â””â”€â”€ page.tsx                  # Home page
â”œâ”€â”€ public/                        # Static assets
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ tailwind.config.ts
â””â”€â”€ next.config.js
```

## Features in Detail

### Chat Interface

- Clean, modern UI with dark theme
- Model name and context window (ctx) display
- Real-time message streaming
- Slot ID tracking for KV cache management

### API Integration

- Streaming chat completions
- Branch conversations with KV cache reuse
- Error handling and retry logic

## Technologies

- **Next.js 15** - React framework
- **React 18** - UI library
- **TypeScript** - Type safety
- **Tailwind CSS** - Styling
- **Server-Sent Events (SSE)** - Real-time streaming

